{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LSTM_TONSORFLOW",
      "provenance": [],
      "authorship_tag": "ABX9TyOFcxUaRAZz4gTYtMo0ThFd",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nazihkhelifa/HW/blob/main/LSTM_TONSOR.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "lFIZlJpwQn-N",
        "outputId": "3b55f880-3900-4bf9-cb55-fcfb5b3f20d4"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-9e8747a1f0b0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    176\u001b[0m \u001b[0;31m####################################################################################\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 178\u001b[0;31m \u001b[0mpredictor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSeriesPredictor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_input\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwindow_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwindow_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_hidden\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_hidden\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    179\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-1-9e8747a1f0b0>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, n_input, window_size, n_hidden)\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_hidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mn_hidden\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mW_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom_normal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mn_hidden\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'W_out'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mb_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom_normal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'b_out'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplaceholder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwindow_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_input\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: module 'tensorflow' has no attribute 'random_normal'"
          ]
        }
      ],
      "source": [
        "\n",
        "# http://github.com/timestocome\n",
        "# data is from  https://www.measuringworth.com/datasets/DJA/index.php\n",
        "\n",
        "\n",
        "# try using a LSTM to predict DJIA using only the DJIA\n",
        "\n",
        "# This converges to a low error rate for both several years, and 1 year \n",
        "# test data looks good, prediction looks terrible.\n",
        "# not a good method to predict stock futures\n",
        "\n",
        "\n",
        "\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt \n",
        "\n",
        "\n",
        "\n",
        "window_size =10            # ~ 5 trading days per week   \n",
        "n_hidden = 44              # ~ 21 trading days per month\n",
        "\n",
        "# select years\n",
        "start_year = 2010           # data is 1900-2017 pick year to start training \n",
        "end_year = 2016\n",
        "\n",
        "\n",
        "# select n most recent days\n",
        "n_days = 300\n",
        "\n",
        "\n",
        "class SeriesPredictor:\n",
        "\n",
        "    def __init__(self, n_input, window_size=window_size, n_hidden=n_hidden):\n",
        "\n",
        "        self.n_input = n_input\n",
        "        self.window_size = window_size\n",
        "        self.n_hidden = n_hidden\n",
        "\n",
        "        self.W_out = tf.Variable(tf.random_normal([n_hidden, 1]), name='W_out')\n",
        "        self.b_out = tf.Variable(tf.random_normal([1]), name='b_out')\n",
        "        self.x = tf.placeholder(tf.float32, [None, window_size, n_input])\n",
        "        self.y = tf.placeholder(tf.float32, [None, window_size])\n",
        "\n",
        "        #self.cost = tf.reduce_mean(tf.square(self.model() - self.y))\n",
        "        self.cost = tf.reduce_mean(tf.abs(self.y - self.model()))\n",
        "        self.train_op = tf.train.AdamOptimizer().minimize(self.cost)\n",
        "\n",
        "        self.saver = tf.train.Saver()\n",
        "\n",
        "\n",
        "\n",
        "    def model(self):\n",
        "\n",
        "        cell = tf.nn.rnn_cell.BasicLSTMCell(self.n_hidden)\n",
        "        outputs, states = tf.nn.dynamic_rnn(cell, self.x, dtype=tf.float32)\n",
        "        n_samples = tf.shape(self.x)[0]\n",
        "        W_repeated = tf.tile(tf.expand_dims(self.W_out, 0), [n_samples, 1, 1])\n",
        "\n",
        "        out = tf.matmul(outputs, W_repeated) + self.b_out\n",
        "        out = tf.squeeze(out)\n",
        "\n",
        "        return out \n",
        "\n",
        "\n",
        "    def train(self, train_x, train_y, test_x, test_y):\n",
        "\n",
        "        with tf.Session() as sess:\n",
        "\n",
        "            tf.get_variable_scope().reuse_variables()\n",
        "            sess.run(tf.global_variables_initializer())\n",
        "\n",
        "            max_patience = 2\n",
        "            patience = max_patience\n",
        "            min_test_err = float('inf')\n",
        "            step = 0\n",
        "            \n",
        "            while patience > 0:\n",
        "                _, train_err = sess.run([self.train_op, self.cost], feed_dict={self.x: train_x, self.y: train_y})\n",
        "            \n",
        "                if step % 100 == 0:\n",
        "            \n",
        "                    test_err = sess.run(self.cost, feed_dict={self.x: test_x, self.y: test_y})\n",
        "                    print('step: {}\\t\\ttrain err: {}\\t\\ttest err: {}'.format(step, train_err, test_err))\n",
        "            \n",
        "                    if test_err < min_test_err:\n",
        "                        min_test_err = test_err\n",
        "                        patience = max_patience\n",
        "                    else:\n",
        "                        patience -= 1\n",
        "                step += 1\n",
        "            \n",
        "            save_path = self.saver.save(sess, 'model.ckpt')\n",
        "            print('Model saved to {}'.format(save_path))\n",
        "\n",
        "\n",
        "    def test(self, sess, test_x):\n",
        "\n",
        "        tf.get_variable_scope().reuse_variables()\n",
        "        self.saver.restore(sess, './model.ckpt')\n",
        "        output = sess.run(self.model(), {self.x: test_x})\n",
        "        \n",
        "        return output\n",
        "\n",
        "#######################################################################\n",
        "\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def load_series(filename, idx=1):\n",
        "\n",
        "    # read file\n",
        "    z = pd.read_csv(filename)\n",
        "    z.columns = ['date', 'value']\n",
        "\n",
        "\n",
        "    # chose data by year\n",
        "    #z['year'] = pd.DatetimeIndex(z.date).year\n",
        "    #z = z[z.year >= start_year]\n",
        "    #z = z[z.year <= end_year]\n",
        "\n",
        "\n",
        "\n",
        "    # chose data by count back from most recent\n",
        "    n_samples = len(z)\n",
        "    start_day = n_samples - n_days\n",
        "    z = z.ix[start_day:n_samples]\n",
        "    print(\"sample count\", len(z))\n",
        "\n",
        "    # format data\n",
        "    data = z['value'].tolist()\n",
        "    normalized_data = (data - np.mean(data)) / np.std(data)\n",
        "\n",
        "    return normalized_data        \n",
        "\n",
        "\n",
        "def split_data(data, percent_train = 0.95):\n",
        "\n",
        "    n_rows = len(data)\n",
        "    train_data, test_data = [], []\n",
        "\n",
        "    for idx, row in enumerate(data):\n",
        "        if idx < n_rows * percent_train:\n",
        "            train_data.append(row)\n",
        "        else:\n",
        "            test_data.append(row)\n",
        "\n",
        "    return train_data, test_data\n",
        "\n",
        "    \n",
        "\n",
        "\n",
        "\n",
        "def plot_results(train_x, predictions, actual, filename):\n",
        "\n",
        "    plt.figure()\n",
        "    num_train = len(train_x)\n",
        "    \n",
        "    plt.plot(list(range(num_train)), train_x, color='b', label='training data')\n",
        "    plt.plot(list(range(num_train, num_train + len(predictions))), predictions, color='r', label='predicted')\n",
        "    plt.plot(list(range(num_train, num_train + len(actual))), actual, color='g', label='test data')\n",
        "\n",
        "    plt.xticks([1,250], ['2016'] )  # ~250 trading days / year\n",
        "\n",
        "    plt.legend(bbox_to_anchor=(1, 0.5))\n",
        "    \n",
        "    if filename is not None:\n",
        "        plt.savefig(filename)\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "\n",
        "####################################################################################\n",
        "\n",
        "predictor = SeriesPredictor(n_input=1, window_size=window_size, n_hidden=n_hidden)\n",
        "\n",
        "\n",
        "data = load_series('DJA.csv')\n",
        "train_data, test_data = split_data(data)\n",
        "\n",
        "train_x, train_y = [], []\n",
        "for i in range(len(train_data) - window_size - 1):\n",
        "    train_x.append(np.expand_dims(train_data[i:i+window_size], axis=1).tolist())\n",
        "    train_y.append(train_data[i+1:i+window_size+1])\n",
        "\n",
        "test_x, test_y = [], []\n",
        "for i in range(len(test_data) - window_size - 1):\n",
        "    test_x.append(np.expand_dims(test_data[i:i+window_size], axis=1).tolist())\n",
        "    test_y.append(test_data[i+1:i+window_size+1])\n",
        "\n",
        "predictor.train(train_x, train_y, test_x, test_y)\n",
        "\n",
        "\n",
        "\n",
        "####################################################################################\n",
        "\n",
        "with tf.Session() as sess:\n",
        "\n",
        "    predicted_values = predictor.test(sess, test_x)[:, 0]\n",
        "    print(\"predictions:\", np.shape(predicted_values))\n",
        "    plot_results(train_data, predicted_values, test_data, 'Predictions_train.png')\n",
        "\n",
        "    previous_sequence = train_x[-1]\n",
        "    predicted_values = []\n",
        "\n",
        "\n",
        "\n",
        "    for i in range(250):            # ~ 1 year\n",
        "\n",
        "        next_sequence = predictor.test(sess, [previous_sequence])\n",
        "        predicted_values.append(next_sequence[-1])\n",
        "        previous_sequence = np.vstack((previous_sequence[1:], next_sequence[-1]))\n",
        "\n",
        "    #print(\"Predicted values:\")\n",
        "    #print(predicted_values)\n",
        "\n",
        "    plot_results(train_data, predicted_values, test_data, 'Predictions_test.png')"
      ]
    }
  ]
}